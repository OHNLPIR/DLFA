{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Code based on:\n",
    "# https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/ and\n",
    "# https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN/\n",
    "\n",
    "# Tested on: Python=2.7, Anaconda 2018.12 OR Keras=2.2.4, TensorFlow=1.13.1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "def clean_str(string):\n",
    "    # Minimal string cleaning for text data\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    # Every dataset is lower cased\n",
    "    return string.strip().lower()\n",
    "\n",
    "data_train = pd.read_csv('data/labeledTrainData_sample.tsv', sep='\\t') # download the full IMDB dataset here: https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n",
    "\n",
    "# read text data to sequences\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for idx in range(data_train.sentiment.shape[0]):\n",
    "    text = BeautifulSoup(data_train.review[idx], \"lxml\")\n",
    "    texts.append(clean_str(text.get_text().encode('ascii', 'ignore')))\n",
    "    labels.append(data_train.sentiment[idx])\n",
    "\n",
    "print(len(texts))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# maximum number of words to keep, based on word frequency\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (10000, 1000))\n",
      "('Shape of label tensor:', (10000, 2))\n"
     ]
    }
   ],
   "source": [
    "# maximal length of sequence\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "# pad input sequences\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# converts a class vector (integers) to binary class matrix.\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "# shuffle data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55198 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive and negative reviews in traing and validation set \n",
      "[4460. 4540.]\n",
      "[487. 513.]\n"
     ]
    }
   ],
   "source": [
    "# training/testing data split\n",
    "VALIDATION_SPLIT = 0.1\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:data.shape[0]-nb_validation_samples]\n",
    "y_train = labels[:data.shape[0]-nb_validation_samples]\n",
    "x_test = data[data.shape[0]-nb_validation_samples:]\n",
    "y_test = labels[data.shape[0]-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set ')\n",
    "print y_train.sum(axis=0)\n",
    "print y_test.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1000, 64)          1280000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,346,202\n",
      "Trainable params: 1,346,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LSTM model.\n",
    "\n",
    "# The first layer is the Embedded layer that uses 32 length vectors to represent each word.\n",
    "embedding_vecor_length = 64\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, embedding_vecor_length, input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "# The next layer is the LSTM layer with 100 neurons.\n",
    "model.add(LSTM(100))\n",
    "\n",
    "# Finally, because this is a classification problem we use a Dense output layer and a sigmoid activation function\n",
    "# to make 0 or 1 predictions for the two classes (good and bad) in the problem.\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# plot neural net architecture\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.eps', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Because it is a binary classification problem, categorical_crossentropy is used as the loss function.\n",
    "# The efficient ADAM optimization algorithm is used.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      "9000/9000 [==============================] - 158s 18ms/step - loss: 0.1424 - acc: 0.9536 - val_loss: 0.4319 - val_acc: 0.8050\n",
      "Epoch 2/3\n",
      "9000/9000 [==============================] - 157s 17ms/step - loss: 0.1733 - acc: 0.9368 - val_loss: 0.4700 - val_acc: 0.8350\n",
      "Epoch 3/3\n",
      "9000/9000 [==============================] - 161s 18ms/step - loss: 0.1002 - acc: 0.9658 - val_loss: 0.5167 - val_acc: 0.8210\n",
      "Accuracy: 82.10%\n"
     ]
    }
   ],
   "source": [
    "# A large batch size of 64 reviews is used to space out weight updates.\n",
    "# The model is fit for 2 epochs because it quickly overfits the problem.\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
